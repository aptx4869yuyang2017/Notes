---
up: 
related:
  - "[[万维钢]]"
created: 2024-09-20
tags:
  - domain/ai
type: "[[Article]]"
finished: 
aliases:
---

9月12日，OpenAI发布了最新的AI模型o1。这是一个影响深远的大事件，我来帮你解说一下。

我之前跟人说，大语言模型革命的第一阶段已经结束。这表现在包括中国的模型和开源模型在内，有多个模型都达到了GPT-4的水平。有时候你比我领先一点，有时候我比你领先一点，但大家都在同一个水平线上 ——
![](https://s1.vika.cn/space/2024/09/20/11ac2dec641842789b02ad0e308b96bc)


**Q\*和草莓就是o1，o1十分强大而且的确有个哲学上无解的危险，缩放定律仍然有效。**



## o1是专门为了解决数学和科学难题而优化的

![](https://s1.vika.cn/space/2024/09/20/d4386b2489c940c9a0fdb0df5c6bf316)



## Transformer + 系统2思考

在于它有了真正的「系统2」的思考。心理学家眼中的系统2就是慢思考，用于**逻辑和复杂决策**。而我们之前讲《智能简史》的时候说过 [6]，**系统2的特点是在做一个决定之前，要在头脑里多模拟几个局面，看看各自的结果如何，然后从中挑选一个最好的作为输出**。

![ed49bc1ff113454ca6efcdba5f836748|563](https://s1.vika.cn/space/2024/09/20/ed49bc1ff113454ca6efcdba5f836748)



o1会先想问题的第一步，对这一步，它产生了比如说四个想法：1）首先我知道月球有多大；2）我知道月亮是奶酪做的；3）一个高尔夫球有多大；4）我不知道怎么回答但我可以猜……

o1自行判断，其中第3个想法最好。于是这就是它思考的第一步。

从第一步出发，o1 再考虑第二步。还是先产生四个想法，从中选择……

这里面有很多细节我们无从得知：每一步到底产生几个想法呢？如何把一个问题拆解成若干个步骤呢？模型如何决定每一步什么时候停止思考？OpenAI刻意保密。

但 OpenAI 明确告诉我们 [8]，这里用的是强化学习。我们讲《智能简史》的时候说过理查德·萨顿的洞见 [9]，强化学习的要点是，不是奖励事情的结果，而要奖励中间的步骤。AlphaGO下围棋是这么做的，o1也是这么做的。



换句话说，o1 等于是把下围棋的方法用于所有的推理问题。这两天我还看到一篇论文 [10]，斯坦福大学的几个很可能是来自 **中国的研究者用数学证明，这套「Transformer + 系统2思考」的方法可以解决“一切”问题！**


## 强化学习会消耗大量Tokne

o1 打开了一个新的烧算力的维度，开启了第二种缩放定律。

![3ca9ecba04fc4057be083365dbe0c8ff|454](https://s1.vika.cn/space/2024/09/20/3ca9ecba04fc4057be083365dbe0c8ff)


而 o1 就不一样。它的预训练也是学习语料，但在**后训练中，除了微调和对齐之外，更重要的是通过强化学习掌握系统2思考的能力**。OpenAI说模型在强化学习阶段主要学三件事：

1. 学会识别并纠正错误；

2. 把复杂的问题分解成更简单的步骤；

3. 如果当前方法无效，会尝试不同的方案。

**这意味着o1的强化学习训练会消耗大量算力。而在模型训练完成后的每一次调用，因为推理过程是系统2思考，每次都要考虑每个步骤的多个想法，所以也要消耗很多算力。**

我看有人做了测试，算个“1加1等于几”，传统模型只需要10到12个tokens，但用 o1（preview版） 算居然消耗了225个tokens。

**目前的大模型都在训练语料，o1已经在训练推理能力了**

> 以前的GPT就如同名校毕业生，上学的时候吃了很多苦学了很多东西，一旦参加工作就不怎么费力思考了，因为做的都是一些比较肤浅的工作。
> 
> o1，则如同是个科学家，上学的苦一点没少吃，可是参加工作以后还得费力思考。因为他面对的都是难题，他不相信直觉，算个1+1都爱想半天……以至于你什么时候看到他，他都是一副身心俱疲的样子。
> 
> 所以这样的人才就是贵啊。ChatGPT中的 o1-preview 模型每星期只让用50次。


## 山姆·奥特曼和伊利亚之间的可能分歧


山姆·奥特曼：结果审查就行了
伊利亚：过程也要审查


## OpenAI 仍有先发优势




[2] https://x.com/howie_serious/status/1835867038713340414

[3] https://www.maximumtruth.org/p/massive-breakthrough-in-ai-intelligence

[4] https://www.youtube.com/watch?v=M9YOO7N5jF8

[7] https://www.interconnects.ai/p/reverse-engineering-openai-o1

[8] https://openai.com/index/learning-to-reason-with-llms/

[10] https://x.com/denny_zhou/status/1835761801453306089；arxiv.org/abs/2402.12875 当然这里说的是数学意义上能解决的问题。

[11] 此图出自 Jim Fan：https://x.com/drjimfan/status/1834279865933332752

[12] https://openai.com/index/openai-o1-system-card/